AWS Bigdata ------- Day 2
Trainer: Prashant Nair 
============================================================================================

Agenda
-------
Storing with realtime stream input using Kinesis and S3
Understanding Kinesis Firehose Configurations
Working with Kinesis Data Streams
Demo on Data Acq and Data Preprocessing in Kinesis Data Streams




Data Acquisition ----> Streaming Data


			 	AWS Kinesis
				     |
		-------------------------------------------------
		|						|
	Data Acquisition				Data Transformation
	        |						|
	--------------------------			AWS Kinesis Analytics
	|			 |
AWS Kinesis 		AWS Kinesis
Firehose		Data Stream



Kinesis Firehose: Is a delivery stream !!! It can only do collect and store !! It's not                   capable to perform data preprocessing.


Kinesis Data Stream: It allows to 
			1. get the realtime stream data, 
			2. perform preprocessing on the fly 
			3. store it in his in-app storage

	This service is very similar to Apache Kafka (Pub-Sub)


Ref: https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html

Steps to Create a Data Stream
=================================

1. Go to Kinesis Dashboard > Click on Kinesis Data Stream > Create Data Stream
2. Set the Stream name  and Click on Create Data Stream

3. Append the below configuration in the agent file

{
  "cloudwatch.emitMetrics": true,
  "kinesis.endpoint": "",
  "firehose.endpoint": "firehose.us-east-1.amazonaws.com",

  "flows": [
    {
      "filePattern": "/var/log/cadabra/*.log",
      "kinesisStream": "CadabraOrders",
      "partitionKeyOption": "RANDOM",
      "dataProcessingOptions": [
         {
            "optionName": "CSVTOJSON",
            "customFieldNames": ["InvoiceNo", "StockCode", "Description", "Quantity", "InvoiceDate", "UnitPrice", "Customer", "Country"]
         }
      ]
    }
  ]
}

4. Restarted the agent

	sudo service aws-kinesis-agent restart


Confirm your presence by saying JOINED in chatbox !!!





End to End Data Acq and Preprocessing Phase
===========================================


InputStream ----> Agent -----> DataStream1 ------> DeliveryStream1 -----> S3

Mini Project 1 -- Flume to S3 using AWS Services
==================================================

Source Dir -->Flume ---> Dest Dir----> Kinesis Agent ---> TransactionDataStream ---> KinesisFirehose ---------> S3
(TransactionDelivery)



Hints:
1. Download Flume

wget https://dlcdn.apache.org/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz

2. Install Flume

tar -xvzf apache-flume-1.9.0-bin.tar.gz

3. Rename the extracted folder

mv apache-flume-1.9.0-bin flume

4. Install Java

sudo yum install -y java-1.8.0-openjdk

5. Check if Java is working

java -version

6. Create source and destination folder

mkdir /home/ec2-user/source
sudo mkdir /var/log/transactions

Give write permission to transaction folder:

sudo chown -R $USER:$GROUP /var/log/transactions


7. Create a configuration for getting the data from /home/ec2-user/source and send it to /var/log/transactions

flumeFolderTrf.conf

#Create Names/Variables for 3 components

ncFileAgent.sources=getFromNetcat
ncFileAgent.channels=goToRAM
ncFileAgent.sinks=writeToFile

#Configure Source i.e. getFromNetcat

ncFileAgent.sources.getFromNetcat.type=spooldir
ncFileAgent.sources.getFromNetcat.spoolDir=/home/ec2-user/source/

#Configure Channel ie. goToRAM

ncFileAgent.channels.goToRAM.type=memory

#Configure Sink

ncFileAgent.sinks.writeToFile.type=file_roll
ncFileAgent.sinks.writeToFile.sink.directory=/var/log/transactions


#Connect the Source with the Channel && Connect the Sink with the Channel

ncFileAgent.sources.getFromNetcat.channels=goToRAM
ncFileAgent.sinks.writeToFile.channel=goToRAM




8. Run Flume

flume/bin/flume-ng agent -n ncFileAgent -f /home/ec2-user/flumeFolderTrf.conf


9. Install AWS Kinesis Agent
10. Configure Kinesis Agent to get data from /var/log/transactions and provide it to TransactionDataStream.

6. Configure the Kinesis Firehose Agent to get the realtime streams from /var/log/cadabra and store the same in my AWS Kinesis delivery stream object. Go to Putty and perform the following

	a. cd /etc/aws-kinesis/
	b. vi agent.json

{
  "cloudwatch.emitMetrics": true,
  "kinesis.endpoint": "",
  "firehose.endpoint": "firehose.us-east-1.amazonaws.com",

  "flows": [
	{
      "filePattern": "/var/log/transactions/*",
      "kinesisStream": "txnsdatastream",
      "partitionKeyOption": "RANDOM",
      "dataProcessingOptions": [
         {
            "optionName": "CSVTOJSON",
            "customFieldNames": ["txnsid", "txndate", "custid", "amount", "category", "subcategory", "city", "state", "txntype"]
         }
      ]
}

{
  "cloudwatch.emitMetrics": true,
  "kinesis.endpoint": "",
  "firehose.endpoint": "firehose.us-east-1.amazonaws.com",

  "flows": [
    {
      "filePattern": "/var/log/transactions/*.log",
      "kinesisStream": "txnsdatastream",
      "partitionKeyOption": "RANDOM",
      "dataProcessingOptions": [
         {
            "optionName": "CSVTOJSON",
            "customFieldNames": ["txnsid", "txndate", "custid", "amount", "category", "subcategory", "city", "state", "txntype"]
         }
      ]
    }
  ]
}

To move data from local system to source:

scp -i ./AWS16Dec2021.pem ./txnsSmall ec2-user@54.211.108.182:/home/ec2-user/source






































